<!DOCTYPE html >
<html>
        <head>
          <title>optimizers - org.platanios.tensorflow.api.ops.training.optimizers</title>
          <meta name="description" content="optimizers - org.platanios.tensorflow.api.ops.training.optimizers" />
          <meta name="keywords" content="optimizers org.platanios.tensorflow.api.ops.training.optimizers" />
          <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
          
      <link href="../../../../../../../lib/template.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../../../../../../lib/diagrams.css" media="screen" type="text/css" rel="stylesheet" id="diagrams-css" />
      <script type="text/javascript" src="../../../../../../../lib/jquery.js" id="jquery-js"></script>
      <script type="text/javascript" src="../../../../../../../lib/jquery-ui.js"></script>
      <script type="text/javascript" src="../../../../../../../lib/template.js"></script>
      <script type="text/javascript" src="../../../../../../../lib/tools.tooltip.js"></script>
      
      <script type="text/javascript">
         if(top === self) {
            var url = '../../../../../../../index.html';
            var hash = 'org.platanios.tensorflow.api.ops.training.optimizers.package';
            var anchor = window.location.hash;
            var anchor_opt = '';
            if (anchor.length >= 1)
              anchor_opt = '@' + anchor.substring(1);
            window.location.href = url + '#' + hash + anchor_opt;
         }
   	  </script>
    
        </head>
        <body class="value">
      <div id="definition">
        <img alt="Package" src="../../../../../../../lib/package_big.png" />
        <p id="owner"><a href="../../../../../../package.html" class="extype" name="org">org</a>.<a href="../../../../../package.html" class="extype" name="org.platanios">platanios</a>.<a href="../../../../package.html" class="extype" name="org.platanios.tensorflow">tensorflow</a>.<a href="../../../package.html" class="extype" name="org.platanios.tensorflow.api">api</a>.<a href="../../package.html" class="extype" name="org.platanios.tensorflow.api.ops">ops</a>.<a href="../package.html" class="extype" name="org.platanios.tensorflow.api.ops.training">training</a></p>
        <h1>optimizers</h1><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      </div>

      <h4 id="signature" class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <span class="name">optimizers</span>
      </span>
      </h4>
      
          <div id="comment" class="fullcommenttop"><div class="comment cmt"></div><dl class="attributes block"> <dt>Source</dt><dd><a href="https://github.com/eaplatanios/tensorflow_scala/tree/master/api/src/main/scala/org/platanios/tensorflow/api/ops/training/optimizers/package.scala" target="_blank">package.scala</a></dd></dl><div class="toggleContainer block">
          <span class="toggle">Linear Supertypes</span>
          <div class="superTypes hiddenContent"><a href="http://www.scala-lang.org/api/2.11.11/index.html#scala.AnyRef" class="extype" target="_top">AnyRef</a>, <a href="http://www.scala-lang.org/api/2.11.11/index.html#scala.Any" class="extype" target="_top">Any</a></div>
        </div></div>
        

      <div id="mbrsel">
        <div id="textfilter"><span class="pre"></span><span class="input"><input id="mbrsel-input" type="text" accesskey="/" /></span><span class="post"></span></div>
        <div id="order">
              <span class="filtertype">Ordering</span>
              <ol>
                
                <li class="alpha in"><span>Alphabetic</span></li>
                <li class="inherit out"><span>By Inheritance</span></li>
              </ol>
            </div>
        <div id="ancestors">
                <span class="filtertype">Inherited<br />
                </span>
                <ol id="linearization">
                  <li class="in" name="org.platanios.tensorflow.api.ops.training.optimizers"><span>optimizers</span></li><li class="in" name="scala.AnyRef"><span>AnyRef</span></li><li class="in" name="scala.Any"><span>Any</span></li>
                </ol>
              </div><div id="ancestors">
            <span class="filtertype"></span>
            <ol>
              <li class="hideall out"><span>Hide All</span></li>
              <li class="showall in"><span>Show All</span></li>
            </ol>
          </div>
        <div id="visbl">
            <span class="filtertype">Visibility</span>
            <ol><li class="public in"><span>Public</span></li><li class="all out"><span>All</span></li></ol>
          </div>
      </div>

      <div id="template">
        <div id="allMembers">
        

        <div id="types" class="types members">
              <h3>Type Members</h3>
              <ol><li name="org.platanios.tensorflow.api.ops.training.optimizers.AMSGrad" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="AMSGradextendsOptimizer"></a>
      <a id="AMSGrad:AMSGrad"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="AMSGrad.html"><span class="name">AMSGrad</span></a><span class="result"> extends <a href="Optimizer.html" class="extype" name="org.platanios.tensorflow.api.ops.training.optimizers.Optimizer">Optimizer</a></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@AMSGradextendsOptimizer" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Optimizer that implements the AMSGrad optimization algorithm, presented in
[On the Convergence of Adam and Beyond](https://openreview.net/pdf?id=ryQu7f-RZ).</p><div class="fullcomment"><div class="comment cmt"><p>Optimizer that implements the AMSGrad optimization algorithm, presented in
[On the Convergence of Adam and Beyond](https://openreview.net/pdf?id=ryQu7f-RZ).</p><p>Initialization:</p><pre>m_0 = <span class="num">0</span>     <span class="cmt">// Initialize the 1st moment vector</span>
v_0 = <span class="num">0</span>     <span class="cmt">// Initialize the 2nd moment vector</span>
v_hat_0 = <span class="num">0</span> <span class="cmt">// Initialize the 2nd moment max vector</span>
t = <span class="num">0</span>       <span class="cmt">// Initialize the time step</span></pre><p>The AMSGrad update for step <code>t</code> is as follows:</p><pre>learningRate_t = initialLearningRate * sqrt(beta1 - beta2^t) / (<span class="num">1</span> - beta1^t)
m_t = beta1 * m_{t-<span class="num">1</span>} + (<span class="num">1</span> - beta1) * gradient
v_t = beta2 * v_{t-<span class="num">1</span>} + (<span class="num">1</span> - beta2) * gradient * gradient
v_hat_t = max(v_t, v_hat_{t-<span class="num">1</span>})
variable -= learningRate_t * m_t / (sqrt(v_hat_t) + epsilon)</pre><p>The default value of <code>1e-8</code> for epsilon might not be a good default in general. For example, when training an
Inception network on ImageNet a current good choice is <code>1.0</code> or <code>0.1</code>.</p><p>The sparse implementation of this algorithm (used when the gradient is an indexed slices object, typically because
of <code>tf.gather</code> or an embedding lookup in the forward pass) does apply momentum to variable slices even if they were
not used in the forward pass (meaning they have a gradient equal to zero). Momentum decay (<code>beta1</code>) is also applied
to the entire momentum accumulator. This means that the sparse behavior is equivalent to the dense behavior (in
contrast to some momentum implementations which ignore momentum unless a variable slice was actually used).</p><p>For more information on this algorithm, please refer to this [paper](https://openreview.net/pdf?id=ryQu7f-RZ).
</p></div></div>
    </li><li name="org.platanios.tensorflow.api.ops.training.optimizers.AdaDelta" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="AdaDeltaextendsOptimizer"></a>
      <a id="AdaDelta:AdaDelta"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="AdaDelta.html"><span class="name">AdaDelta</span></a><span class="result"> extends <a href="Optimizer.html" class="extype" name="org.platanios.tensorflow.api.ops.training.optimizers.Optimizer">Optimizer</a></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@AdaDeltaextendsOptimizer" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Optimizer that implements the AdaDelta optimization algorithm.</p><div class="fullcomment"><div class="comment cmt"><p>Optimizer that implements the AdaDelta optimization algorithm.</p><p>The AdaDelta update is as follows:</p><pre>accumulator = rho * accumulator + (<span class="num">1</span> - rho) * gradient
update = sqrt(accumulatorUpdate + epsilon) * rsqrt(accumulator + epsilon) * gradient
accumulatorUpdate = rho * accumulatorUpdate + (<span class="num">1</span> - rho) * square(update)
variable -= update</pre><p>For more information on this algorithm, please refer to this [paper](http://arxiv.org/abs/1212.5701)
([PDF](http://arxiv.org/pdf/1212.5701v1.pdf)).
</p></div></div>
    </li><li name="org.platanios.tensorflow.api.ops.training.optimizers.AdaGrad" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="AdaGradextendsOptimizer"></a>
      <a id="AdaGrad:AdaGrad"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="AdaGrad.html"><span class="name">AdaGrad</span></a><span class="result"> extends <a href="Optimizer.html" class="extype" name="org.platanios.tensorflow.api.ops.training.optimizers.Optimizer">Optimizer</a></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@AdaGradextendsOptimizer" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Optimizer that implements the AdaGrad optimization algorithm.</p><div class="fullcomment"><div class="comment cmt"><p>Optimizer that implements the AdaGrad optimization algorithm.</p><p>The AdaGrad update is as follows:</p><pre>accumulator += gradient * gradient
variable -= stepSize * gradient * (<span class="num">1</span> / sqrt(accumulator))</pre><p>For more information on this algorithm, please refer to this
[paper](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf).
</p></div></div>
    </li><li name="org.platanios.tensorflow.api.ops.training.optimizers.Adam" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="AdamextendsOptimizer"></a>
      <a id="Adam:Adam"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="Adam.html"><span class="name">Adam</span></a><span class="result"> extends <a href="Optimizer.html" class="extype" name="org.platanios.tensorflow.api.ops.training.optimizers.Optimizer">Optimizer</a></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@AdamextendsOptimizer" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Optimizer that implements the Adam optimization algorithm.</p><div class="fullcomment"><div class="comment cmt"><p>Optimizer that implements the Adam optimization algorithm.</p><p>Initialization:</p><pre>m_0 = <span class="num">0</span>  <span class="cmt">// Initialize the 1st moment vector</span>
v_0 = <span class="num">0</span>  <span class="cmt">// Initialize the 2nd moment vector</span>
t = <span class="num">0</span>    <span class="cmt">// Initialize the time step</span></pre><p>The Adam update for step <code>t</code> is as follows:</p><pre>learningRate_t = initialLearningRate * sqrt(beta1 - beta2^t) / (<span class="num">1</span> - beta1^t)
m_t = beta1 * m_{t-<span class="num">1</span>} + (<span class="num">1</span> - beta1) * gradient
v_t = beta2 * v_{t-<span class="num">1</span>} + (<span class="num">1</span> - beta2) * gradient * gradient
variable -= learningRate_t * m_t / (sqrt(v_t) + epsilon)</pre><p>The default value of <code>1e-8</code> for epsilon might not be a good default in general. For example, when training an
Inception network on ImageNet a current good choice is <code>1.0</code> or <code>0.1</code>. Note that since the Adam optimizer uses the
formulation just before Section 2.1 of the [Kingma and Ba paper](https://arxiv.org/abs/1412.6980) rather than the
formulation in Algorithm 1, the &quot;epsilon&quot; referred to here is &quot;epsilon hat&quot; in the paper.</p><p>The sparse implementation of this algorithm (used when the gradient is an indexed slices object, typically because
of <code>tf.gather</code> or an embedding lookup in the forward pass) does apply momentum to variable slices even if they were
not used in the forward pass (meaning they have a gradient equal to zero). Momentum decay (<code>beta1</code>) is also applied
to the entire momentum accumulator. This means that the sparse behavior is equivalent to the dense behavior (in
contrast to some momentum implementations which ignore momentum unless a variable slice was actually used).</p><p>For more information on this algorithm, please refer to this [paper](https://arxiv.org/abs/1412.6980)
([PDF](https://arxiv.org/pdf/1412.6980.pdf)).
</p></div></div>
    </li><li name="org.platanios.tensorflow.api.ops.training.optimizers.GradientDescent" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="GradientDescentextendsOptimizer"></a>
      <a id="GradientDescent:GradientDescent"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="GradientDescent.html"><span class="name">GradientDescent</span></a><span class="result"> extends <a href="Optimizer.html" class="extype" name="org.platanios.tensorflow.api.ops.training.optimizers.Optimizer">Optimizer</a></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@GradientDescentextendsOptimizer" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Optimizer that implements the gradient descent algorithm and includes support for learning rate decay, momentum, and
Nesterov acceleration.</p>
    </li><li name="org.platanios.tensorflow.api.ops.training.optimizers.LazyAMSGrad" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="LazyAMSGradextendsAMSGrad"></a>
      <a id="LazyAMSGrad:LazyAMSGrad"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="LazyAMSGrad.html"><span class="name">LazyAMSGrad</span></a><span class="result"> extends <a href="AMSGrad.html" class="extype" name="org.platanios.tensorflow.api.ops.training.optimizers.AMSGrad">AMSGrad</a></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@LazyAMSGradextendsAMSGrad" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Optimizer that implements a variant of the AMSGrad optimization algorithm that handles sparse updates more
efficiently.</p><div class="fullcomment"><div class="comment cmt"><p>Optimizer that implements a variant of the AMSGrad optimization algorithm that handles sparse updates more
efficiently.</p><p>The original AMSGrad algorithm maintains three moving-average accumulators for each trainable variable; the
accumulators are updated at every step. This class provides lazier handling of gradient updates for sparse
variables. It only updates the moving-average accumulators for sparse variable indices that appear in the current
batch, rather than updating the accumulators for all indices. Compared with the original Adam optimizer, it can
provide large improvements in model training throughput for some applications. However, it provides slightly
different semantics than the original Adam algorithm, and may lead to different empirical results.</p><p>Initialization:</p><pre>m_0 = <span class="num">0</span>     <span class="cmt">// Initialize the 1st moment vector</span>
v_0 = <span class="num">0</span>     <span class="cmt">// Initialize the 2nd moment vector</span>
v_hat_0 = <span class="num">0</span> <span class="cmt">// Initialize the 2nd moment max vector</span>
t = <span class="num">0</span>       <span class="cmt">// Initialize the time step</span></pre><p>The AMSGrad update for step <code>t</code> is as follows:</p><pre>learningRate_t = initialLearningRate * sqrt(beta1 - beta2^t) / (<span class="num">1</span> - beta1^t)
m_t = beta1 * m_{t-<span class="num">1</span>} + (<span class="num">1</span> - beta1) * gradient
v_t = beta2 * v_{t-<span class="num">1</span>} + (<span class="num">1</span> - beta2) * gradient * gradient
v_hat_t = max(v_t, v_hat_{t-<span class="num">1</span>})
variable -= learningRate_t * m_t / (sqrt(v_hat_t) + epsilon)</pre><p>The default value of <code>1e-8</code> for epsilon might not be a good default in general. For example, when training an
Inception network on ImageNet a current good choice is <code>1.0</code> or <code>0.1</code>.</p><p>The sparse implementation of this algorithm (used when the gradient is an indexed slices object, typically because
of <code>tf.gather</code> or an embedding lookup in the forward pass) does not apply momentum to variable slices if they are
not used in the forward pass (meaning they have a gradient equal to zero). Momentum decay (<code>beta1</code>) is also not
applied to the entire momentum accumulator. This means that the sparse behavior is not equivalent to the dense
behavior.</p><p>For more information on this algorithm, please refer to this [paper](https://openreview.net/pdf?id=ryQu7f-RZ).
</p></div></div>
    </li><li name="org.platanios.tensorflow.api.ops.training.optimizers.LazyAdam" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="LazyAdamextendsAdam"></a>
      <a id="LazyAdam:LazyAdam"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="LazyAdam.html"><span class="name">LazyAdam</span></a><span class="result"> extends <a href="Adam.html" class="extype" name="org.platanios.tensorflow.api.ops.training.optimizers.Adam">Adam</a></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@LazyAdamextendsAdam" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Optimizer that implements a variant of the Adam optimization algorithm that handles sparse updates more efficiently.</p><div class="fullcomment"><div class="comment cmt"><p>Optimizer that implements a variant of the Adam optimization algorithm that handles sparse updates more efficiently.</p><p>The original Adam algorithm maintains two moving-average accumulators for each trainable variable; the accumulators
are updated at every step. This class provides lazier handling of gradient updates for sparse variables. It only
updates the moving-average accumulators for sparse variable indices that appear in the current batch, rather than
updating the accumulators for all indices. Compared with the original Adam optimizer, it can provide large
improvements in model training throughput for some applications. However, it provides slightly different semantics
than the original Adam algorithm, and may lead to different empirical results.</p><p>Initialization:</p><pre>m_0 = <span class="num">0</span>  <span class="cmt">// Initialize the 1st moment vector</span>
v_0 = <span class="num">0</span>  <span class="cmt">// Initialize the 2nd moment vector</span>
t = <span class="num">0</span>    <span class="cmt">// Initialize the time step</span></pre><p>The Adam update for step <code>t</code> is as follows:</p><pre>learningRate_t = initialLearningRate * sqrt(beta1 - beta2^t) / (<span class="num">1</span> - beta1^t)
m_t = beta1 * m_{t-<span class="num">1</span>} + (<span class="num">1</span> - beta1) * gradient
v_t = beta2 * v_{t-<span class="num">1</span>} + (<span class="num">1</span> - beta2) * gradient * gradient
variable -= learningRate_t * m_t / (sqrt(v_t) + epsilon)</pre><p>The default value of <code>1e-8</code> for epsilon might not be a good default in general. For example, when training an
Inception network on ImageNet a current good choice is <code>1.0</code> or <code>0.1</code>. Note that since the Adam optimizer uses the
formulation just before Section 2.1 of the [Kingma and Ba paper](https://arxiv.org/abs/1412.6980) rather than the
formulation in Algorithm 1, the &quot;epsilon&quot; referred to here is &quot;epsilon hat&quot; in the paper.</p><p>The sparse implementation of this algorithm (used when the gradient is an indexed slices object, typically because
of <code>tf.gather</code> or an embedding lookup in the forward pass) does not apply momentum to variable slices if they are
not used in the forward pass (meaning they have a gradient equal to zero). Momentum decay (<code>beta1</code>) is also not
applied to the entire momentum accumulator. This means that the sparse behavior is not equivalent to the dense
behavior.</p><p>For more information on the original Adam algorithm, please refer to this [paper](https://arxiv.org/abs/1412.6980)
([PDF](https://arxiv.org/pdf/1412.6980.pdf)).</p><p>The original Adam algorithm (described above) maintains two moving-average accumulators for each trainable variable,
which are updated at every step. This class provides a lazier handling of gradient updates for sparse variables. It
only updates moving-average accumulators for sparse variable indices that appear in the current batch, rather than
updating the accumulators for all indices. Compared with the original Adam optimizer, it can provide large
improvements in model training throughput for some applications. However, it provides slightly different semantics
than the original Adam algorithm, and may lead to different empirical results.
</p></div></div>
    </li><li name="org.platanios.tensorflow.api.ops.training.optimizers.Optimizer" visbl="pub" data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="OptimizerextendsAnyRef"></a>
      <a id="Optimizer:Optimizer"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a href="Optimizer.html"><span class="name">Optimizer</span></a><span class="result"> extends <a href="http://www.scala-lang.org/api/2.11.11/index.html#scala.AnyRef" class="extype" target="_top">AnyRef</a></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@OptimizerextendsAnyRef" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt"></p>
    </li><li name="org.platanios.tensorflow.api.ops.training.optimizers.RMSProp" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="RMSPropextendsOptimizer"></a>
      <a id="RMSProp:RMSProp"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="RMSProp.html"><span class="name">RMSProp</span></a><span class="result"> extends <a href="Optimizer.html" class="extype" name="org.platanios.tensorflow.api.ops.training.optimizers.Optimizer">Optimizer</a></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@RMSPropextendsOptimizer" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Optimizer that implements the RMSProp optimization algorithm.</p><div class="fullcomment"><div class="comment cmt"><p>Optimizer that implements the RMSProp optimization algorithm.</p><p>The RMSProp update is as follows:</p><pre>rmsAcc = decay * rmsAcc + (<span class="num">1</span> - decay) * (gradient ^ <span class="num">2</span>)
momAcc = momentum * momAcc + learningRate * gradient / sqrt(rmsAcc + epsilon)
variable -= momAcc</pre><p>This implementation of RMSProp uses plain momentum, not Nesterov momentum.</p><p>If the centered version is used, the algorithm additionally maintains a moving (discounted) average of the
gradients, and uses that average to estimate the variance:</p><pre>meanGradAcc = decay * rmsAcc + (<span class="num">1</span> - decay) * gradient
rmsAcc = decay * rmsAcc + (<span class="num">1</span> - decay) * (gradient ^ <span class="num">2</span>)
momAcc = momentum * momAcc + learningRate * gradient / sqrt(rmsAcc - (meanGradAcc ^ <span class="num">2</span>) + epsilon)
variable -= momAcc</pre></div></div>
    </li><li name="org.platanios.tensorflow.api.ops.training.optimizers.YellowFin" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="YellowFinextendsGradientDescent"></a>
      <a id="YellowFin:YellowFin"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a href="YellowFin.html"><span class="name">YellowFin</span></a><span class="result"> extends <a href="GradientDescent.html" class="extype" name="org.platanios.tensorflow.api.ops.training.optimizers.GradientDescent">GradientDescent</a></span>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@YellowFinextendsGradientDescent" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Optimizer that implements the YellowFin algorithm.</p><div class="fullcomment"><div class="comment cmt"><p>Optimizer that implements the YellowFin algorithm.</p><p>Please refer to [Zhang et. al., 2017](https://arxiv.org/abs/1706.03471) for details.
</p></div></div>
    </li></ol>
            </div>

        

        <div id="values" class="values members">
              <h3>Value Members</h3>
              <ol><li name="org.platanios.tensorflow.api.ops.training.optimizers.AMSGrad" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="AMSGrad"></a>
      <a id="AMSGrad:AMSGrad"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="AMSGrad$.html"><span class="name">AMSGrad</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@AMSGrad" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li><li name="org.platanios.tensorflow.api.ops.training.optimizers.AdaDelta" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="AdaDelta"></a>
      <a id="AdaDelta:AdaDelta"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="AdaDelta$.html"><span class="name">AdaDelta</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@AdaDelta" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li><li name="org.platanios.tensorflow.api.ops.training.optimizers.AdaGrad" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="AdaGrad"></a>
      <a id="AdaGrad:AdaGrad"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="AdaGrad$.html"><span class="name">AdaGrad</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@AdaGrad" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li><li name="org.platanios.tensorflow.api.ops.training.optimizers.Adam" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="Adam"></a>
      <a id="Adam:Adam"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="Adam$.html"><span class="name">Adam</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@Adam" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li><li name="org.platanios.tensorflow.api.ops.training.optimizers.GradientDescent" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="GradientDescent"></a>
      <a id="GradientDescent:GradientDescent"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="GradientDescent$.html"><span class="name">GradientDescent</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@GradientDescent" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li><li name="org.platanios.tensorflow.api.ops.training.optimizers.LazyAMSGrad" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="LazyAMSGrad"></a>
      <a id="LazyAMSGrad:LazyAMSGrad"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="LazyAMSGrad$.html"><span class="name">LazyAMSGrad</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@LazyAMSGrad" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li><li name="org.platanios.tensorflow.api.ops.training.optimizers.LazyAdam" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="LazyAdam"></a>
      <a id="LazyAdam:LazyAdam"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="LazyAdam$.html"><span class="name">LazyAdam</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@LazyAdam" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li><li name="org.platanios.tensorflow.api.ops.training.optimizers.RMSProp" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="RMSProp"></a>
      <a id="RMSProp:RMSProp"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="RMSProp$.html"><span class="name">RMSProp</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@RMSProp" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li><li name="org.platanios.tensorflow.api.ops.training.optimizers.YellowFin" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="YellowFin"></a>
      <a id="YellowFin:YellowFin"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a href="YellowFin$.html"><span class="name">YellowFin</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@YellowFin" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      
    </li><li name="org.platanios.tensorflow.api.ops.training.optimizers.schedules" visbl="pub" data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="schedules"></a>
      <a id="schedules:schedules"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a href="schedules/package.html"><span class="name">schedules</span></a>
      </span>
      </h4><span class="permalink">
      <a href="../../../../../../../index.html#org.platanios.tensorflow.api.ops.training.optimizers.package@schedules" title="Permalink" target="_top">
        <img src="../../../../../../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt"></p>
    </li></ol>
            </div>

        

        
        </div>

        <div id="inheritedMembers">
        <div class="parent" name="scala.AnyRef">
              <h3>Inherited from <a href="http://www.scala-lang.org/api/2.11.11/index.html#scala.AnyRef" class="extype" target="_top">AnyRef</a></h3>
            </div><div class="parent" name="scala.Any">
              <h3>Inherited from <a href="http://www.scala-lang.org/api/2.11.11/index.html#scala.Any" class="extype" target="_top">Any</a></h3>
            </div>
        
        </div>

        <div id="groupedMembers">
        <div class="group" name="Ungrouped">
              <h3>Ungrouped</h3>
              
            </div>
        </div>

      </div>

      <div id="tooltip"></div>

      <div id="footer">  </div>


    </body>
      </html>
